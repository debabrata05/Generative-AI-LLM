{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNWjpHK+I+RHKhGD8bYnuCA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"78500df8ee6142cd84aa5f10e8a23b21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a004eb611b7342038dfa8a936a7ff819","IPY_MODEL_f25e0b5bd2694a31a9e10cb0aca5e69c","IPY_MODEL_3eadeb59650941b68a2d3959003feb1e"],"layout":"IPY_MODEL_62d675976e05470786ba8b35ffb667fa"}},"a004eb611b7342038dfa8a936a7ff819":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0c8ec89aad24a0db791eb57161d885a","placeholder":"​","style":"IPY_MODEL_6b9335957f5c444b8891a8251cfe50da","value":"100%"}},"f25e0b5bd2694a31a9e10cb0aca5e69c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92f52646b2f34ca1ae2051e6611c0eae","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4ce55254a9443a381022da840434b8b","value":3}},"3eadeb59650941b68a2d3959003feb1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e88321fd300466c92c933a0da433020","placeholder":"​","style":"IPY_MODEL_e252a155619c48bea57ff56f8ff16605","value":" 3/3 [00:00&lt;00:00, 103.40it/s]"}},"62d675976e05470786ba8b35ffb667fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c8ec89aad24a0db791eb57161d885a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b9335957f5c444b8891a8251cfe50da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92f52646b2f34ca1ae2051e6611c0eae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ce55254a9443a381022da840434b8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e88321fd300466c92c933a0da433020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e252a155619c48bea57ff56f8ff16605":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wnAS8vbxMzg","executionInfo":{"status":"ok","timestamp":1704951012399,"user_tz":-330,"elapsed":33421,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"f549d32d-0bba-4b83-f91b-594a77dc6437"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Usage:   \n","  pip install [options] <requirement specifier> [package-index-options] ...\n","  pip install [options] -r <requirements file> [package-index-options] ...\n","  pip install [options] [-e] <vcs project url> ...\n","  pip install [options] [-e] <local project path> ...\n","  pip install [options] <archive url/path> ...\n","\n","no such option: --quite\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","Usage:   \n","  pip install [options] <requirement specifier> [package-index-options] ...\n","  pip install [options] -r <requirements file> [package-index-options] ...\n","  pip install [options] [-e] <vcs project url> ...\n","  pip install [options] [-e] <local project path> ...\n","  pip install [options] <archive url/path> ...\n","\n","no such option: --quite\n"]}],"source":["%pip install --upgrade pip --quite\n","%pip install --disable-pip-version-check \\\n","    torch==1.13.1 \\\n","    torchdata==0.5.1 --quiet\n","\n","%pip install \\\n","    transformers==4.27.2 \\\n","    datasets==2.11.0 \\\n","    evaluate==0.4.0 \\\n","    rouge_score==0.1.2 \\\n","    peft==0.3.0 \\\n","    trl==0.4.4 --quiet\n","\n","!pip install tensorflow --quite\n","# !pip uninstall -y transformers accelerate --quite\n","# !pip install transformers accelerate --quite\n","# !unzip \"/content/peft-dialogue-summary-checkpoint-local.zip\" -d \"/content/peft-dialogue-summary-checkpoint-local\""]},{"cell_type":"code","source":["!pip install -U tensorflow==2.10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FppygFtHBI31","executionInfo":{"status":"ok","timestamp":1704951437158,"user_tz":-330,"elapsed":56356,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"be2c7494-f541-4174-ebb1-22fc1caf8366"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.10\n","  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (23.5.26)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.60.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (3.9.0)\n","Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10)\n","  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.10)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (23.2)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.16.0)\n","Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10)\n","  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (0.35.0)\n","Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10)\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.2)\n","Installing collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.14.0\n","    Uninstalling tensorflow-estimator-2.14.0:\n","      Successfully uninstalled tensorflow-estimator-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","google","keras","tensorboard","tensorflow"]}}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n","from datasets import load_dataset\n","from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n","\n","# trl: Transformer Reinforcement Learning library\n","from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n","from trl import create_reference_model\n","from trl.core import LengthSampler\n","\n","import torch\n","import evaluate\n","\n","import numpy as np\n","import pandas as pd\n","\n","# tqdm library makes the loops show a smart progress meter.\n","from tqdm import tqdm\n","tqdm.pandas()"],"metadata":{"id":"KxkrhCxxxiT0","executionInfo":{"status":"ok","timestamp":1704951437158,"user_tz":-330,"elapsed":16,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator"],"metadata":{"id":"tzw91r_cxuAL"}},{"cell_type":"code","source":["model_name=\"google/flan-t5-base\"\n","huggingface_dataset_name = \"knkarthick/dialogsum\"\n","\n","dataset_original = load_dataset(huggingface_dataset_name)\n","\n","dataset_original"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433,"referenced_widgets":["78500df8ee6142cd84aa5f10e8a23b21","a004eb611b7342038dfa8a936a7ff819","f25e0b5bd2694a31a9e10cb0aca5e69c","3eadeb59650941b68a2d3959003feb1e","62d675976e05470786ba8b35ffb667fa","d0c8ec89aad24a0db791eb57161d885a","6b9335957f5c444b8891a8251cfe50da","92f52646b2f34ca1ae2051e6611c0eae","d4ce55254a9443a381022da840434b8b","2e88321fd300466c92c933a0da433020","e252a155619c48bea57ff56f8ff16605"]},"id":"UecxPUbhxtIt","executionInfo":{"status":"ok","timestamp":1704951103829,"user_tz":-330,"elapsed":3691,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"c6392476-b283-4b25-da09-466b95e97d48"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78500df8ee6142cd84aa5f10e8a23b21"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 12460\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1500\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 500\n","    })\n","})"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def build_dataset(model_name,\n","                  dataset_name,\n","                  input_min_text_length,\n","                  input_max_text_length):\n","\n","    \"\"\"\n","    Preprocess the dataset and split it into train and test parts.\n","\n","    Parameters:\n","    - model_name (str): Tokenizer model name.\n","    - dataset_name (str): Name of the dataset to load.\n","    - input_min_text_length (int): Minimum length of the dialogues.\n","    - input_max_text_length (int): Maximum length of the dialogues.\n","\n","    Returns:\n","    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n","    \"\"\"\n","\n","    # load dataset (only \"train\" part will be enough for this lab).\n","    dataset = load_dataset(dataset_name, split=\"train\")\n","\n","    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n","    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n","\n","    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n","\n","    def tokenize(sample):\n","\n","        # Wrap each dialogue with the instruction.\n","        prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{sample[\"dialogue\"]}\n","\n","Summary:\n","\"\"\"\n","        sample[\"input_ids\"] = tokenizer.encode(prompt)\n","\n","        # This must be called \"query\", which is a requirement of our PPO library.\n","        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n","        return sample\n","\n","    # Tokenize each dialogue.\n","    dataset = dataset.map(tokenize, batched=False)\n","    dataset.set_format(type=\"torch\")\n","\n","    # Split the dataset into train and test parts.\n","    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n","\n","    return dataset_splits\n","\n","dataset = build_dataset(model_name=model_name,\n","                        dataset_name=huggingface_dataset_name,\n","                        input_min_text_length=200,\n","                        input_max_text_length=1000)\n","\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozJHe2a-xyUU","executionInfo":{"status":"ok","timestamp":1704951106057,"user_tz":-330,"elapsed":1249,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"fc92e04b-1dfe-4a7d-d8ae-50bc1c375f6d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-23525d6e58aeae0c.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c346cf9bd39677f9.arrow\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n","        num_rows: 8017\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n","        num_rows: 2005\n","    })\n","})\n"]}]},{"cell_type":"code","source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"],"metadata":{"id":"s96JAWKYyQFt","executionInfo":{"status":"ok","timestamp":1704951109756,"user_tz":-330,"elapsed":645,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["lora_config = LoraConfig(\n","    r=8, # Rank\n","    lora_alpha=32,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",")\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n","                                              torch_dtype=torch.bfloat16)\n","\n","peft_model = PeftModel.from_pretrained(model,\n","                                       './peft-dialogue-summary-checkpoint-local/peft-dialogue-summary-checkpoint-local',\n","                                       lora_config=lora_config,\n","                                       torch_dtype=torch.bfloat16,\n","                                       device_map=\"auto\",\n","                                       is_trainable=True)\n","\n","print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CefBRpU6yRnb","executionInfo":{"status":"ok","timestamp":1704951119710,"user_tz":-330,"elapsed":8593,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"0784ca9c-cbac-4320-d0e8-d01b0d92a68f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["PEFT model parameters to be updated:\n","\n","trainable model parameters: 884736\n","all model parameters: 248462592\n","percentage of trainable model parameters: 0.36%\n","\n"]}]},{"cell_type":"code","source":["ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n","                                                               torch_dtype=torch.bfloat16,\n","                                                               is_trainable=True)\n","\n","print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n","print(ppo_model.v_head)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oY1IUFnhytKM","executionInfo":{"status":"ok","timestamp":1704951119711,"user_tz":-330,"elapsed":6,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"650c4e00-c09d-43ac-aebb-95c44ae3b38c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["PPO model parameters to be updated (ValueHead + 769 params):\n","\n","trainable model parameters: 885505\n","all model parameters: 248463361\n","percentage of trainable model parameters: 0.36%\n","\n","ValueHead(\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (summary): Linear(in_features=768, out_features=1, bias=True)\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n",")\n"]}]},{"cell_type":"code","source":["ref_model = create_reference_model(ppo_model)\n","\n","print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6oKwhyNzTFz","executionInfo":{"status":"ok","timestamp":1704951122682,"user_tz":-330,"elapsed":709,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"2fd2be6b-ba20-4d1e-f20f-3dbd6b8985e7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Reference model parameters to be updated:\n","\n","trainable model parameters: 0\n","all model parameters: 248463361\n","percentage of trainable model parameters: 0.00%\n","\n"]}]},{"cell_type":"code","source":["toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n","toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n","print(toxicity_model.config.id2label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjGkc7pwzWhT","executionInfo":{"status":"ok","timestamp":1704951126068,"user_tz":-330,"elapsed":2108,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"48ae670c-a6f6-4cc3-8365-335e48bd57e3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'nothate', 1: 'hate'}\n"]}]},{"cell_type":"code","source":["non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n","toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n","toxicity_model.to('cpu')\n","logits = toxicity_model(input_ids=toxicity_input_ids).logits\n","print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n","\n","# Print the probabilities for [not hate, hate]\n","probabilities = logits.softmax(dim=-1).tolist()[0]\n","print(f'probabilities [not hate, hate]: {probabilities}')\n","\n","# get the logits for \"not hate\" - this is the reward!\n","not_hate_index = 0\n","nothate_reward = (logits[:, not_hate_index]).tolist()\n","print(f'reward (high): {nothate_reward}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htkxDNSKzaOr","executionInfo":{"status":"ok","timestamp":1704951142577,"user_tz":-330,"elapsed":4,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"9103b0e7-37bb-4643-876c-464716dc4b0f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["logits [not hate, hate]: [3.114102602005005, -2.489619255065918]\n","probabilities [not hate, hate]: [0.9963293671607971, 0.0036706042010337114]\n","reward (high): [3.114102602005005]\n"]}]},{"cell_type":"code","source":["toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n","\n","toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n","\n","logits = toxicity_model(toxicity_input_ids).logits\n","print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n","\n","# Print the probabilities for [not hate, hate]\n","probabilities = logits.softmax(dim=-1).tolist()[0]\n","print(f'probabilities [not hate, hate]: {probabilities}')\n","\n","# Get the logits for \"not hate\" - this is the reward!\n","nothate_reward = (logits[:, not_hate_index]).tolist()\n","print(f'reward (low): {nothate_reward}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8LprMxszccb","executionInfo":{"status":"ok","timestamp":1704951144125,"user_tz":-330,"elapsed":5,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"4f77b3b3-e613-42eb-9d57-c0f107dcc043"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["logits [not hate, hate]: [-0.6921197175979614, 0.3722734749317169]\n","probabilities [not hate, hate]: [0.25647082924842834, 0.743529200553894]\n","reward (low): [-0.6921197175979614]\n"]}]},{"cell_type":"code","source":["device = 0 if torch.cuda.is_available() else \"cpu\"\n","sentiment_pipe = pipeline(\"sentiment-analysis\",\n","                          model=toxicity_model_name,\n","                          device=device)\n","reward_logits_kwargs = {\n","    \"top_k\": None, # Return all scores.\n","    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n","    \"batch_size\": 16\n","}\n","\n","reward_probabilities_kwargs = {\n","    \"top_k\": None, # Return all scores.\n","    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n","    \"batch_size\": 16\n","}\n","\n","print(\"Reward model output:\")\n","print(\"For non-toxic text\")\n","print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n","print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n","print(\"For toxic text\")\n","print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n","print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"id":"77QCyTVUzfPb","executionInfo":{"status":"error","timestamp":1704951437904,"user_tz":-330,"elapsed":762,"user":{"displayName":"DEBABRATA KUMAR KARAN","userId":"05140082950719781588"}},"outputId":"194d6817-afc7-4faa-b005-6f96c3010467"},"execution_count":19,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nNo module named 'keras.saving.legacy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0;31m from ...modeling_tf_utils import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mTFCausalLanguageModelingLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.legacy'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-58468a31ced3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m sentiment_pipe = pipeline(\"sentiment-analysis\", \n\u001b[0m\u001b[1;32m      3\u001b[0m                           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoxicity_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           device=device)\n\u001b[1;32m      5\u001b[0m reward_logits_kwargs = {\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# Will load the correct model if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlook_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                     \u001b[0m_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"TF{architecture}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1129\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nNo module named 'keras.saving.legacy'"]}]},{"cell_type":"code","source":["print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n","print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"],"metadata":{"id":"BNwBaqYXzh1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n","print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"],"metadata":{"id":"AsmOx__3zkVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["toxicity_evaluator = evaluate.load(\"toxicity\",\n","                                    toxicity_model_name,\n","                                    module_type=\"measurement\",\n","                                    toxic_label=\"hate\")"],"metadata":{"id":"s20PvCoYzmOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["toxicity_score = toxicity_evaluator.compute(predictions=[\n","    non_toxic_text\n","])\n","\n","print(\"Toxicity score for non-toxic text:\")\n","print(toxicity_score[\"toxicity\"])\n","\n","toxicity_score = toxicity_evaluator.compute(predictions=[\n","    toxic_text\n","])\n","\n","print(\"\\nToxicity score for toxic text:\")\n","print(toxicity_score[\"toxicity\"])"],"metadata":{"id":"pw1_KHI7zoYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_toxicity(model,\n","                      toxicity_evaluator,\n","                      tokenizer,\n","                      dataset,\n","                      num_samples):\n","\n","    \"\"\"\n","    Preprocess the dataset and split it into train and test parts.\n","\n","    Parameters:\n","    - model (trl model): Model to be evaluated.\n","    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n","    - tokenizer (transformers tokenizer): Tokenizer to be used.\n","    - dataset (dataset): Input dataset for the evaluation.\n","    - num_samples (int): Maximum number of samples for the evaluation.\n","\n","    Returns:\n","    tuple: A tuple containing two numpy.float64 values:\n","    - mean (numpy.float64): Mean of the samples toxicity.\n","    - std (numpy.float64): Standard deviation of the samples toxicity.\n","    \"\"\"\n","\n","    max_new_tokens=100\n","\n","    toxicities = []\n","    input_texts = []\n","    for i, sample in tqdm(enumerate(dataset)):\n","        input_text = sample[\"query\"]\n","\n","        if i > num_samples:\n","            break\n","\n","        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n","\n","        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n","                                             tok_k=0.0,\n","                                             top_p=1.0,\n","                                             do_sample=True)\n","\n","        response_token_ids = model.generate(input_ids=input_ids,\n","                                            generation_config=generation_config)\n","\n","        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n","\n","        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n","\n","        toxicities.extend(toxicity_score[\"toxicity\"])\n","\n","    # Compute mean & std using np.\n","    mean = np.mean(toxicities)\n","    std = np.std(toxicities)\n","\n","    return mean, std"],"metadata":{"id":"MTO5Di4XzqUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n","\n","mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n","                                                                          toxicity_evaluator=toxicity_evaluator,\n","                                                                          tokenizer=tokenizer,\n","                                                                          dataset=dataset[\"test\"],\n","                                                                          num_samples=10)\n","\n","print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"],"metadata":{"id":"_A4FMkxtzvGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collator(data):\n","    return dict((key, [d[key] for d in data]) for key in data[0])\n","\n","test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n","print(f'Collator input: {test_data}')\n","print(f'Collator output: {collator(test_data)}')"],"metadata":{"id":"m6X6MubBzxOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate=1.41e-5\n","max_ppo_epochs=1\n","mini_batch_size=4\n","batch_size=16\n","\n","config = PPOConfig(\n","    model_name=model_name,\n","    learning_rate=learning_rate,\n","    ppo_epochs=max_ppo_epochs,\n","    mini_batch_size=mini_batch_size,\n","    batch_size=batch_size\n",")\n","\n","ppo_trainer = PPOTrainer(config=config,\n","                         model=ppo_model,\n","                         ref_model=ref_model,\n","                         tokenizer=tokenizer,\n","                         dataset=dataset[\"train\"],\n","                         data_collator=collator)"],"metadata":{"id":"2k5i3T8az3Mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_min_length = 100\n","output_max_length = 400\n","output_length_sampler = LengthSampler(output_min_length, output_max_length)\n","\n","generation_kwargs = {\n","    \"min_length\": 5,\n","    \"top_k\": 0.0,\n","    \"top_p\": 1.0,\n","    \"do_sample\": True\n","}\n","\n","reward_kwargs = {\n","    \"top_k\": None, # Return all scores.\n","    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n","    \"batch_size\": 16\n","}\n","\n","max_ppo_steps = 10\n","\n","for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n","    # Break when you reach max_steps.\n","    if step >= max_ppo_steps:\n","        break\n","\n","    prompt_tensors = batch[\"input_ids\"]\n","\n","    # Get response from FLAN-T5/PEFT LLM.\n","    summary_tensors = []\n","\n","    for prompt_tensor in prompt_tensors:\n","        max_new_tokens = output_length_sampler()\n","\n","        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n","        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n","\n","        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n","\n","    # This needs to be called \"response\".\n","    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n","\n","    # Compute reward outputs.\n","    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n","    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n","\n","    # You use the `nothate` item because this is the score for the positive `nothate` class.\n","    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n","\n","    # Run PPO step.\n","    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n","    ppo_trainer.log_stats(stats, batch, reward_tensors)\n","\n","    print(f'objective/kl: {stats[\"objective/kl\"]}')\n","    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n","    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n","    print('-'.join('' for x in range(100)))"],"metadata":{"id":"l1aJtcBUz39j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model,\n","                                                                        toxicity_evaluator=toxicity_evaluator,\n","                                                                        tokenizer=tokenizer,\n","                                                                        dataset=dataset[\"test\"],\n","                                                                        num_samples=10)\n","print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"],"metadata":{"id":"cTTPhZMuz78z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n","std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n","\n","print(f'Percentage improvement of toxicity score after detoxification:')\n","print(f'mean: {mean_improvement*100:.2f}%')\n","print(f'std: {std_improvement*100:.2f}%')"],"metadata":{"id":"O-7-DVyZ0BSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 20\n","compare_results = {}\n","\n","df_batch = dataset[\"test\"][0:batch_size]\n","\n","compare_results[\"query\"] = df_batch[\"query\"]\n","prompt_tensors = df_batch[\"input_ids\"]\n","\n","summary_tensors_ref = []\n","summary_tensors = []\n","\n","# Get response from ppo and base model.\n","for i in tqdm(range(batch_size)):\n","    gen_len = output_length_sampler()\n","    generation_kwargs[\"max_new_tokens\"] = gen_len\n","\n","    summary = ref_model.generate(\n","        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n","        **generation_kwargs\n","    ).squeeze()[-gen_len:]\n","    summary_tensors_ref.append(summary)\n","\n","    summary = ppo_model.generate(\n","        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n","        **generation_kwargs\n","    ).squeeze()[-gen_len:]\n","    summary_tensors.append(summary)\n","\n","# Decode responses.\n","compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n","compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n","\n","# Sentiment analysis of query/response pairs before/after.\n","texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n","rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n","compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n","\n","texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n","rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n","compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"],"metadata":{"id":"3XdllyKZ0GDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.set_option('display.max_colwidth', 500)\n","df_compare_results = pd.DataFrame(compare_results)\n","df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n","df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n","df_compare_results_sorted"],"metadata":{"id":"DBv3yWJu0GwN"},"execution_count":null,"outputs":[]}]}